{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка-данных\" data-toc-modified-id=\"Подготовка-данных-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка данных</a></span><ul class=\"toc-item\"><li><span><a href=\"#Загрузка-и-импорт-библиотек-и-функций\" data-toc-modified-id=\"Загрузка-и-импорт-библиотек-и-функций-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Загрузка и импорт библиотек и функций</a></span></li><li><span><a href=\"#Загрузка-и-изучение-данных-из-файла\" data-toc-modified-id=\"Загрузка-и-изучение-данных-из-файла-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Загрузка и изучение данных из файла</a></span></li><li><span><a href=\"#Предобработка-текста\" data-toc-modified-id=\"Предобработка-текста-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Предобработка текста</a></span></li><li><span><a href=\"#Создание-эмбеддингов\" data-toc-modified-id=\"Создание-эмбеддингов-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Создание эмбеддингов</a></span></li><li><span><a href=\"#Подготовка-признаков\" data-toc-modified-id=\"Подготовка-признаков-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Подготовка признаков</a></span></li></ul></li><li><span><a href=\"#Построение-моделей\" data-toc-modified-id=\"Построение-моделей-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Построение моделей</a></span><ul class=\"toc-item\"><li><span><a href=\"#Модель-логистической-регрессии\" data-toc-modified-id=\"Модель-логистической-регрессии-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Модель логистической регрессии</a></span></li><li><span><a href=\"#Модель-&quot;Случайный-лес&quot;\" data-toc-modified-id=\"Модель-&quot;Случайный-лес&quot;-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Модель \"Случайный лес\"</a></span></li><li><span><a href=\"#Модель-LightGBM\" data-toc-modified-id=\"Модель-LightGBM-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Модель LightGBM</a></span></li></ul></li><li><span><a href=\"#Тестирование-моделей\" data-toc-modified-id=\"Тестирование-моделей-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Тестирование моделей</a></span></li><li><span><a href=\"#Общий-вывод\" data-toc-modified-id=\"Общий-вывод-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Общий вывод</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификация и поиск негативных комментариев (с BERT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "В проекте требуется создать модель для классификации комментариев на позитивные и негативные (токсичные). Для работы предоставлен набор данных с разметкой о токсичности правок. Построенная модель должна иметь значение метрики качества *F1* не меньше 0.75. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка и импорт библиотек и функций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\programdata\\anaconda3\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from lightgbm) (1.7.1)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from lightgbm) (1.20.3)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from lightgbm) (1.1.2)\n",
      "Requirement already satisfied: wheel in c:\\programdata\\anaconda3\\lib\\site-packages (from lightgbm) (0.37.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (2.2.0)\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (1.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\programdata\\anaconda3\\lib\\site-packages (4.22.1)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.3.1)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2021.8.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (21.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.9.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Раскомментить только при необходимости! При установке библиотек будет подкачиваться 2-3 Гб информации. Процесс не быстрый\n",
    "#!pip install --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/cu117 -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import transformers as tfs\n",
    "from tqdm import notebook\n",
    "import time\n",
    "import re\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка и изучение данных из файла"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим данные из файла и ознакомимся с таблицей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 159292 entries, 0 to 159450\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159292 non-null  object\n",
      " 1   toxic   159292 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    main_data = pd.read_csv('toxic_comments.csv', index_col=[0])\n",
    "except:\n",
    "    main_data = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "main_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь посмотрим непосредственно на сами данные. Изменим отображаемую по умолчанию ширину столбцов в pandas, чтобы увидеть больше текстовой информации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember what page that's on?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the tools well.  · talk \"</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Your vandalism to the Matt Shirvington article has been reverted.  Please don't do it again, or you will be banned.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sorry if the word 'nonsense' was offensive to you. Anyway, I'm not intending to write anything in the article(wow they would jump on me for vandalism), I'm merely requesting that it be more encyclopedic so one can use it for school as a reference. I have been to the selective breeding page but it's almost a stub. It points to 'animal breeding' which is a short messy article that gives you no info. There must be someone around with expertise in eugenics? 93.161.107.169</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>alignment on this subject and which are contrary to those of DuLithgow</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                           Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    D'aww! He matches this background colour I'm seemingly stuck with. Thanks.  (talk) 21:51, January 11, 2016 (UTC)   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                           Hey man, I'm really not trying to edit war. It's just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page. He seems to care more about the formatting than the actual info.   \n",
       "3  \"\\nMore\\nI can't make any real suggestions on improvement - I wondered if the section statistics should be later on, or a subsection of \"\"types of accidents\"\"  -I think the references may need tidying so that they are all in the exact same format ie date format etc. I can do that later on, if no-one else does first - if you have any preferences for formatting style on references or want to do it yourself please let me know.\\n\\nThere appears to be a backlog on articles for review so I guess there may be a delay until a reviewer turns up. It's listed in the relevant form eg Wikipedia:Good_article_nominations#Transport  \"   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 You, sir, are my hero. Any chance you remember what page that's on?   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \"\\n\\nCongratulations from me as well, use the tools well.  · talk \"   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK   \n",
       "7                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Your vandalism to the Matt Shirvington article has been reverted.  Please don't do it again, or you will be banned.   \n",
       "8                                                                                                                                                            Sorry if the word 'nonsense' was offensive to you. Anyway, I'm not intending to write anything in the article(wow they would jump on me for vandalism), I'm merely requesting that it be more encyclopedic so one can use it for school as a reference. I have been to the selective breeding page but it's almost a stub. It points to 'animal breeding' which is a short messy article that gives you no info. There must be someone around with expertise in eugenics? 93.161.107.169   \n",
       "9                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              alignment on this subject and which are contrary to those of DuLithgow   \n",
       "\n",
       "   toxic  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  \n",
       "5      0  \n",
       "6      1  \n",
       "7      0  \n",
       "8      0  \n",
       "9      0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = 800\n",
    "main_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные отображены, можно приступать к обработке текста."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предобработка текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оставим в тексте только слова на латинице. Остальные символы заменим пробелами. Оформим код в виде функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    text_list = re.sub(r'[^a-zA-Z]', ' ', text).split()\n",
    "    return \" \".join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data['text'] = main_data['text'].apply(clear_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь следует привести весь текст к нижнему регистру. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data['text'] = main_data['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>explanation why the edits made under my username hardcore metallica fan were reverted they weren t vandalisms just closure on some gas after i voted at new york dolls fac and please don t remove the template from the talk page since i m retired now</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d aww he matches this background colour i m seemingly stuck with thanks talk january utc</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hey man i m really not trying to edit war it s just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page he seems to care more about the formatting than the actual info</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>more i can t make any real suggestions on improvement i wondered if the section statistics should be later on or a subsection of types of accidents i think the references may need tidying so that they are all in the exact same format ie date format etc i can do that later on if no one else does first if you have any preferences for formatting style on references or want to do it yourself please let me know there appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up it s listed in the relevant form eg wikipedia good article nominations transport</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you sir are my hero any chance you remember what page that s on</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>congratulations from me as well use the tools well talk</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cocksucker before you piss around on my work</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        text  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                   explanation why the edits made under my username hardcore metallica fan were reverted they weren t vandalisms just closure on some gas after i voted at new york dolls fac and please don t remove the template from the talk page since i m retired now   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   d aww he matches this background colour i m seemingly stuck with thanks talk january utc   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                      hey man i m really not trying to edit war it s just that this guy is constantly removing relevant information and talking to me through edits instead of my talk page he seems to care more about the formatting than the actual info   \n",
       "3  more i can t make any real suggestions on improvement i wondered if the section statistics should be later on or a subsection of types of accidents i think the references may need tidying so that they are all in the exact same format ie date format etc i can do that later on if no one else does first if you have any preferences for formatting style on references or want to do it yourself please let me know there appears to be a backlog on articles for review so i guess there may be a delay until a reviewer turns up it s listed in the relevant form eg wikipedia good article nominations transport   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            you sir are my hero any chance you remember what page that s on   \n",
       "5                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    congratulations from me as well use the tools well talk   \n",
       "6                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               cocksucker before you piss around on my work   \n",
       "\n",
       "   toxic  \n",
       "0      0  \n",
       "1      0  \n",
       "2      0  \n",
       "3      0  \n",
       "4      0  \n",
       "5      0  \n",
       "6      1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_data.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразования текста прошли верно.\n",
    "\n",
    "Выполним теперь поиск дубликатов в тексте и удалим их, если они есть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1294"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_data.drop_duplicates(inplace=True)\n",
    "main_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дубликаты успешно удалились, но проверим отдельно наличие дубликатов только в столбце с текстом без учёта других."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_data['text'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данная ситуация говорит о том, что в таблице есть одинаковые тексты, но с разной оценкой токсичности. Такие тексты следует удалить, чтобы не запутывать модель. Отфильтруем такие дубликаты вместе с исходными строками, для чего применим параметр *keep=False*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    }
   ],
   "source": [
    "print(main_data['text'].duplicated(keep=False).sum())\n",
    "main_data = main_data[~main_data['text'].duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(157940, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Фильтрация проша успешно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание эмбеддингов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для перевода текста в векторное представление (эмбеддинги) воспользуемся одним из аналогов модели BERT - DistilBERT. Данная модель весит на 40% меньше, чем оригинальная BERT-модель, работает на 60% быстрее ее и сохраняет 97% ее функциональности. Веса модели и конфигурации используем из готовой предобученной на токсичность текста модели *dapang/distilbert-base-uncased-finetuned-toxicity*.\n",
    "\n",
    "Модель построим с помощью средств библиотеки transformers и torch. До создания эмбеддингов произведём токенизацию текста и преобразование в формат тензоров. Все действия оформим в виде функции, которая будет в дальнейшем создавать преобразованные обучающие признаки (в формате Dataframe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_embeddings(features): \n",
    "    #Создание устройства, для подключения видеокарты к расчётам\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "    # Создание токенизатора и токенизация текста\n",
    "    tokenizer = tfs.DistilBertTokenizer.from_pretrained('dapang/distilbert-base-uncased-finetuned-toxicity')\n",
    "    tokenized= features.apply(lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=512, truncation=True))\n",
    "    \n",
    "    #Поиск длины наибольшего вектора\n",
    "    max_len = 0\n",
    "    for i in tokenized.values:\n",
    "        if len(i) > max_len:\n",
    "            max_len = len(i)\n",
    "\n",
    "    #Создание отступов и масок\n",
    "    padded = np.array([i + [0]*(max_len - len(i)) for i in tokenized.values])\n",
    "    attention_mask = np.where(padded != 0, 1, 0)\n",
    "\n",
    "    #Создание модели\n",
    "    model = tfs.DistilBertModel.from_pretrained('dapang/distilbert-base-uncased-finetuned-toxicity')\n",
    "    model = model.to(device)\n",
    "    batch_size = 50\n",
    "\n",
    "    #Создание эмбеддингов\n",
    "    embeddings = []\n",
    "    for i in notebook.tqdm(range(padded.shape[0] // batch_size)):\n",
    "        batch = torch.cuda.LongTensor(padded[batch_size*i:batch_size*(i+1)]) \n",
    "        attention_mask_batch = torch.cuda.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)])\n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "        batch_embedding = batch_embeddings[0][:,0,:]\n",
    "        embeddings.append(batch_embedding.cpu().numpy())\n",
    "\n",
    "    features_converted = np.concatenate(embeddings)\n",
    "    features_converted = pd.DataFrame(features_converted)\n",
    "    return features_converted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того чтобы эмбеддинги не создавались слишком долго, сделаем случайную выборку размером 20000 из исходных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_p = main_data.sample(20000, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_p.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выборка создалась корректно. Можно приступать к созданию эмбеддингов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dapang/distilbert-base-uncased-finetuned-toxicity were not used when initializing DistilBertModel: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e47042001bd4d6187ddcd4802412f48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "features = make_embeddings(data_p['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь подготовим обучающие и тестовые признаки. Данные с обычными (не целевыми) признаками уже были подготовлены в предыдущем пункте. Создадим целевой признак. Перед разделением на выборки исследуем баланс классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    18042\n",
       "1     1958\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_p['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имеет место дисбаланс классов, единичных значений почти в 9 раз меньше. Этот факт надо учесть при создании выборки. Воспользуемся параметром *stratify* в функции *train_test_split*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер обучающей выборки признаков: (12000, 768)\n",
      "Размер обучающей выборки целевых признаков: (12000,)\n",
      "Размер валидационной выборки признаков: (4000, 768)\n",
      "Размер валидационной выборки целевых признаков: (4000,)\n",
      "Размер тестовой выборки признаков: (4000, 768)\n",
      "Размер тестовой выборки целевых признаков: (4000,)\n"
     ]
    }
   ],
   "source": [
    "target = data_p['toxic'].reset_index(drop=True)\n",
    "\n",
    "#выделим тестовую выборку размером в 20%\n",
    "train_valid_features, test_features, train_valid_target, test_target = train_test_split(\n",
    "    features, target, test_size=0.2, stratify=target, random_state=123)\n",
    "\n",
    "#выделим валидационную выборку размером в 20% от исходной выборки (test_size = 0.2/0.8)\n",
    "train_features, valid_features, train_target, valid_target = train_test_split(\n",
    "    train_valid_features, train_valid_target, test_size=0.25, stratify=train_valid_target, random_state=123)\n",
    "\n",
    "print('Размер обучающей выборки признаков:', train_features.shape)\n",
    "print('Размер обучающей выборки целевых признаков:', train_target.shape)\n",
    "print('Размер валидационной выборки признаков:', valid_features.shape)\n",
    "print('Размер валидационной выборки целевых признаков:', valid_target.shape)\n",
    "print('Размер тестовой выборки признаков:', test_features.shape)\n",
    "print('Размер тестовой выборки целевых признаков:', test_target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как имеет место дисбаланс значений целевого признака, то выполним увеличение обучающей выборки, добавив объекты с единичными значениями целевого признака."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20225, 768)\n",
      "(20225,)\n",
      "0    10825\n",
      "1     9400\n",
      "Name: toxic, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def upsample(features, target, repeat):\n",
    "    features_zeros = features[target == 0]\n",
    "    features_ones = features[target == 1]\n",
    "    target_zeros = target[target == 0]\n",
    "    target_ones = target[target == 1]\n",
    "    features_upsampled = pd.concat([features_zeros] + [features_ones] * repeat)\n",
    "    target_upsampled = pd.concat([target_zeros] + [target_ones] * repeat)\n",
    "    features_upsampled, target_upsampled = shuffle(features_upsampled, target_upsampled, random_state=123)\n",
    "    return features_upsampled, target_upsampled\n",
    "\n",
    "train_features_ups, train_target_ups = upsample(train_features, train_target, 8)\n",
    "print(train_features_ups.shape)\n",
    "print(train_target_ups.shape)\n",
    "print(train_target_ups.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Увеличение выборки выполнено корректно. Подготовка признаков на этом закончена, можно приступать к построению моделей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Построение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель логистической регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим модель логистической регрессии и в цикле подберём один из её гиперпараметров. Качество модели проверим на валидационной выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_values = list(np.linspace(0.001, 100, 10))\n",
    "params_grid = {'C': c_values,\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "for c_value in params_grid['C']:\n",
    "    params = {\n",
    "        'solver': 'liblinear',\n",
    "        'C': c_value,\n",
    "        #'class_weight': 'balanced',\n",
    "        'random_state': 123\n",
    "    }\n",
    "    model = LogisticRegression(**params)\n",
    "    model.fit(train_features_ups, train_target_ups)\n",
    "    predictions = model.predict(valid_features)\n",
    "    result_score = f1_score(valid_target, predictions)\n",
    "    if result_score > best_score:\n",
    "        best_params = params\n",
    "        best_score = result_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Среднее значение метрики F1:  0.7362514029180697\n",
      "Лучшие параметры модели:\n",
      "{'solver': 'liblinear', 'C': 33.333999999999996, 'random_state': 123}\n"
     ]
    }
   ],
   "source": [
    "print('Среднее значение метрики F1: ', best_score)\n",
    "print('Лучшие параметры модели:')\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним все параметры модели в словаре, чтобы использовать в разделе 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_voc = {}\n",
    "best_params_voc['logistic'] = best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** модель логистической регрессии показала неплохой результат. Но всё равно рассмотрим далее другие модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель \"Случайный лес\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим модель \"Случайный лес\" и в цикле подберём её гиперпараметры. Качество модели проверим на валидационной выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = {'n_estimators': range(10,101,10),\n",
    "               'max_depth': range(1,10,1)\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 55.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "for n_estimators in params_grid['n_estimators']:\n",
    "    for max_depth in params_grid['max_depth']:\n",
    "        params = {\n",
    "            'n_estimators': n_estimators,\n",
    "            'max_depth': max_depth,\n",
    "            #'class_weight': 'balanced',\n",
    "            'random_state': 123,\n",
    "            'n_jobs': -1\n",
    "            }\n",
    "        model = RandomForestClassifier(**params)\n",
    "        model.fit(train_features_ups, train_target_ups)\n",
    "        predictions = model.predict(valid_features)\n",
    "        result_score = f1_score(valid_target, predictions)\n",
    "        if result_score > best_score:\n",
    "            best_params = params\n",
    "            best_score = result_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Значение метрики F1: 0.7479674796747968\n",
      "Лучшие параметры модели:\n",
      "{'n_estimators': 10, 'max_depth': 9, 'random_state': 123, 'n_jobs': -1}\n"
     ]
    }
   ],
   "source": [
    "print('Значение метрики F1:', best_score)\n",
    "print('Лучшие параметры модели:')\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним все параметры модели в словаре, чтобы использовать в разделе 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_voc['forest'] = best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** модель \"Случайный лес\" показала ожидаемо ещё более высокий результат, чем модель логистической регрессии. Рассмотрим далее модель градиентного бустинга."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модель LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим модель LightGBM и в цикле подберём её гиперпараметы. Качество модели проверим на валидационной выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = {'n_estimators': [500],\n",
    "              'num_leaves': [27, 29, 30],\n",
    "              'learning_rate':[0.06, 0.07, 0.09]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "best_score = 0\n",
    "best_params = {}\n",
    "for n_estimators in params_grid['n_estimators']:\n",
    "    for num_leaves in params_grid['num_leaves']:\n",
    "        for learning_rate in params_grid['learning_rate']:\n",
    "            params = {\n",
    "                'n_estimators': n_estimators,\n",
    "                'num_leaves': num_leaves,\n",
    "                'learning_rate': learning_rate,\n",
    "                #'class_weight': 'balanced',\n",
    "                'random_state': 123\n",
    "                }\n",
    "            model = LGBMClassifier(**params)\n",
    "            model.fit(train_features_ups, train_target_ups)\n",
    "            predictions = model.predict(valid_features)\n",
    "            result_score = f1_score(valid_target, predictions)\n",
    "            if result_score > best_score:\n",
    "                best_params = params\n",
    "                best_score = result_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Значение метрики F1: 0.772169167803547\n",
      "Лучшие параметры модели:\n",
      "{'n_estimators': 500, 'num_leaves': 27, 'learning_rate': 0.09, 'random_state': 123}\n"
     ]
    }
   ],
   "source": [
    "print('Значение метрики F1:', best_score)\n",
    "print('Лучшие параметры модели:')\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним все параметры модели в словаре, чтобы использовать в разделе 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_voc['lightgbm'] = best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** модель LightGBM лучший результ среди всех моделей. Требуемое значение метрики F1 достигнуто."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирование моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Гиперпараметры для рассмотренных моделей успешно подобраны.Также получены значения метрики F1 на валидационной выборке. Теперь проверим работу моделей на тестовой выборке и получим уже другие значения метрики F1, а также посчитаем время обучения и предсказания для каждой модели.\n",
    "\n",
    "Для удобства создадим таблицу для хранения результатов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = pd.DataFrame({'Модель': [], 'F1-мера':[], 'Время обучения, с': [], 'Время предсказания, с':[]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cоздадим функцию, которая будет обучать модель, получать предсказания по **тестовой** выборке и рассчитывать время этих операций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_routine(model, train_features, train_target, test_features, test_target):\n",
    "    time1 = time.time()\n",
    "    model.fit(train_features, train_target)\n",
    "    time2 = time.time()\n",
    "    fit_time = time2 - time1\n",
    "\n",
    "    time1 = time.time()\n",
    "    predictions = model.predict(test_features)\n",
    "    time2 = time.time()\n",
    "    predict_time = time2 - time1 \n",
    "    \n",
    "    f1_value = f1_score(test_target, predictions)\n",
    "    \n",
    "    return f1_value, fit_time, predict_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Последовательно построим модели с подобранными гиперпараметрами, посчитаем необходимые величины и занесём их в таблицу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(**best_params_voc['logistic'])\n",
    "f1_value, fit_time, predict_time = model_routine(model, train_features_ups, train_target_ups, test_features, test_target)\n",
    "model_results.loc[0] = ['Logistic Regression', f1_value, fit_time, predict_time]\n",
    "\n",
    "model = RandomForestClassifier(**best_params_voc['forest'])\n",
    "f1_value, fit_time, predict_time = model_routine(model, train_features_ups, train_target_ups, test_features, test_target)\n",
    "model_results.loc[1] = ['Random Forest Classifier', f1_value, fit_time, predict_time]\n",
    "\n",
    "model = LGBMClassifier(**best_params_voc['lightgbm'])\n",
    "f1_value, fit_time, predict_time = model_routine(model, train_features_ups, train_target_ups, test_features, test_target)\n",
    "model_results.loc[2] = ['LightGBM', f1_value, fit_time, predict_time]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим теперь таблицу с численными результатами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Модель</th>\n",
       "      <th>F1-мера</th>\n",
       "      <th>Время обучения, с</th>\n",
       "      <th>Время предсказания, с</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.713178</td>\n",
       "      <td>19.544148</td>\n",
       "      <td>0.015617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>0.730455</td>\n",
       "      <td>0.250017</td>\n",
       "      <td>0.015626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.762689</td>\n",
       "      <td>6.979536</td>\n",
       "      <td>0.031252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Модель   F1-мера  Время обучения, с  \\\n",
       "0       Logistic Regression  0.713178          19.544148   \n",
       "1  Random Forest Classifier  0.730455           0.250017   \n",
       "2                  LightGBM  0.762689           6.979536   \n",
       "\n",
       "   Время предсказания, с  \n",
       "0               0.015617  \n",
       "1               0.015626  \n",
       "2               0.031252  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** наибольшее значение F1-меры на тестовой выборке показала модель LightGBM. При этом у данной модели не самое большое время обучения. По условиям задачи значение F1-меры должно быть не менее 0,75. Модель LightGBM превысила это значение, поэтому выбираем эту модель для поставленной в проекте задачи."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общий вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В проекте были загружены и изучены данные с текстовыми комментариями. \n",
    "\n",
    "Произведена предобработка текста: текст очищен от лишних символов кроме латинских букв, все символы приведены к нижнему регистру, удалены дубликаты.\n",
    "\n",
    "Далее текст переведён в векторное представление (эмбеддинги) с помощью модели DistilBERT на основе предобученной модели для токсичных текстов. Ввиду ограниченности производительной мощности, для создания эмбеддингов была использована уменьшенная выборка из исходных данных. \n",
    "\n",
    "Следующим шагом подготовлены признаки для обучения моделей: выборка разделена на обучающую и тестовую, выделены обучающие и целевые признаки, проведено увеличение выборки из-за дисбаланса классов.\n",
    "\n",
    "Затем было произведено построение трёх моделей: модель логистической регрессии, модель \"Случайный лес\" и модель градиентного бустинга LightGBM. Предсказания моделей выполнялись на обучающей выборке, а метрика F1 считалась на валидационной выборке. Наилучшие результаты по качеству предсказания показала модель LightGBM. \n",
    "\n",
    "Далее была проверена работа всех моделей на тестовой выборке. Наилучший результат по метрике F1 показала модель LightGBM. При этом результат превышает требуемое в проекте значение 0,75.\n",
    "\n",
    "Для поставленной в проекте задачи выбрана модель LightGBM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Содержание",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
